---
layout: null
---

# 1 向量与向量空间

## 1.1 向量的定义

- 定义：向量是一个具有**大小**和**方向**的量（相对于标量，即只具有大小的量）。在物理学视角中可以是速度，在计算机视角中可以是数组。
- 代数表示：
  - 二维：$f{v} = (x, y)$
  - 三维：$f{v} = (x, y, z)$
  - $n$维：$v=(v_{1},v_{2},…,v_{n})$
- 几何意义：可视为从原点指向点 $v=(v_{1},v_{2},…,v_{n})$的箭头。
- 表示方式：
  行向量：$$\begin{bmatrix}&v_1, &v_2, &..., &v_n  \end{bmatrix}$$
  列向量：$$\begin{bmatrix}
 \\v_1
 \\v_2
 \\⋮
 \\v_n
\end{bmatrix}$$
- 向量的维度：**行数\*列数**
- 注：从向量空间来看，将同一个向量表达为行向量或列向量的形式只是写法不同，表达的还是同一个抽象对象，但严格来说，他们之间是转置关系。在线性代数应用中，默认用**列向量**表示向量。同维的向量才能相加
- 在平面中的点表示为向量，如将点$(x，y)$表示为向量$$\begin{bmatrix}
 \\x
 \\y
\end{bmatrix}$$

## 1.2 单位向量

在$x$轴方向上 $\hat{i} = (1,0)$, $\hat{j} = (0,1)$，分别代表每个正方向上模长为一的向量。二维情况下，平面上的向量都可以表示成两个单位向量分别乘上系数再相加的结果，不过也有例外。

例如：$\overrightarrow{a} = (8,12) = 8\hat{i} + 12\hat{j}$

单位向量的求法：$\hat{v} = \frac{\overrightarrow{v}}{\left \| \overrightarrow{v} \right \|}$

## 1.3 向量的基本运算

### 1.3.1 向量加减法

- 加法：$$(𝑎_{1},𝑎_{2},…,𝑎_{𝑛})+(𝑏_{1},𝑏_{2},…,𝑏_{𝑛})=(𝑎_{1}+𝑏_{1},𝑎_{2}+𝑏_{2},…,𝑎_{𝑛}+𝑏_{𝑛})$$
  几何意义：平行四边形法则。

- 减法：
  $$(𝑎_1,𝑎_2,…,𝑎_𝑛)−(𝑏_1,𝑏_2,…,𝑏_𝑛)=(𝑎_1−𝑏_1,𝑎_2−𝑏_2,…,𝑎_𝑛−𝑏_𝑛)$$

### 1.3.2 向量数乘

$$c⋅(v_1,v_2,…,v_n)=(cv_1,cv_2,…,cv_n)$$

几何意义：改变向量长度|c|倍，方向由 c 的符号决定：正号向原方向延展，负号向原方向的反方向延展。

## 1.4 向量的模长

- 模长：
  $$|v|=\sqrt{v_1^2 + v_2^2 + \dots + v_n^2}$$

几何意义：向量箭头的长度。

- 范数（norm）：向量的范数是长度的推广，相当于标量的绝对值。在二维情况下相当于向量的模长。计算方式：$$\left \| v \right \|_2 = \sqrt{v^2_1+v^2_2+...+v^2_n}$$
  - 绝对值：$\left |x \right |$
  - 范数： $\left \| \overrightarrow{v} \right \|$

## 1.5 向量的内积(点积，点乘)

- 定义：
  $$\mathbf{u} \cdot \mathbf{v} = u_1 v_1 + u_2 v_2 + \dots + u_n v_n$$
- 几何意义：
  $$\mathbf{v} = |\mathbf{u}| \, |\mathbf{v}| \cos \theta$$
  其中 $\theta$ 是两向量夹角。
- 应用：判断正交（$\mathbf{u}\cdot\mathbf{v}=0 ⇔ 垂直$），物理上的功，计算向量投影
- 总结：**点积是两个向量在同一方向上的“重合程度”**

## 1.6 向量的外积(三维)

- 右手定则：当你用右手的拇指、食指、中指互相垂直伸开时：食指指向第一个向量 a 的方向，中指指向第二个向量 b 的方向，拇指所指方向就是叉积 $\mathbf{a} \times \mathbf{b}$ 的方向。几何意义上它规定了三维空间中一组互相垂直的三个方向的“旋向”，即所谓的 **右手坐标系**。
- 右手坐标系：是**人为规定**的，物理界和数学界统一选择的一个“正方向”，事实上左手系也是可以的，不过约定上只使用右手系。同时，自然界中的很多现象（如电磁波传播时，电场、磁场和传播方向的关系）天然符合右手系。如果我们用左手系来描述，就会显得“反常”。

外积（叉积/向量积）的定义：$$a\times b = \left | a \right | \left | b \right | \sin{\theta} n$$

- $\theta$：$a,b$的夹角
- $n$：垂直于 $a,b$所在平面的单位向量，方向由**右手定则**确定
- 叉积**不符合交换律**
- 两个平行的向量叉积的结果为 0，一个向量与其本身叉积的结果为 0

几何意义：

- 方向：结果向量垂直于$a,b$所在平面
- 长度：等于以$a,b$为边的平行四边形面积
- 零向量情况：如果$a,b$平行或其中一个为零，则叉积为零

物理意义：力矩，角动量，洛伦兹力，磁通量

总结：**叉积就是两个作用量共同产生的效应**

## 1.7 线性组合与线性相关

### 1.7.1 线性组合

定义：给定一组向量$v_1,v_2,...,v_k$，它们的线性组合是指$$w = c_1v_1 + c_2v_2 + ... + c_kv_k$$
其中 c 是标量，给每个向量乘上一个系数。简单来说，线性组合就是把几个向量分别乘上系数再加到一起，结果是一个，或一组（因为随着取的系数不同，最终产生的向量也会不同）在向量空间内的新向量。线性组合要求所有的向量**同维**，即在同一个向量空间中，不能组合不同维度的向量。

以二维空间举例，如果有向量$a=(1,2),b=(0,3)$，对其进行线性组合，如，分别乘上系数$3,2$，则结果是$$w = 3(1,2) + 2(0,3) = (3,12)$$而随着取不同的系数，最终取得的向量也会不同。

“线性”的含义：数学上的线性有两个条件：可加性和齐次性：

- 可加性：$f(c+u) = f(c)+f(u)$
- 齐次性：$f(cu)=cf(u)$
  简单来说，只涉及加法和乘法，不涉及平方。

同时，在二维情况下，对几个向量进行线性组合意味着平移，拉伸，叠加，不涉及其他的操作。

### 1.7.2 线性相关

定义：给定一组向量$v_1,v_2,...v_k$，如果存在一组**不全为零**的系数$c_1,c_2,...c_k$，使得$$c_1v_1 + c_2v_2 + ... + c_kv_k = 0$$那么这组向量就是**线性相关**的。

如果只有一组解$c_1=c_2=...=c_k=0$，那么这组向量就是**线性无关**的。

通俗解释，线性相关就是“某个向量能被其他向量拼出来”；线性无关就是“没有一个向量能被其他向量拼出来”，线性相关的一组向量中有“多余“的向量，而线性无关的一组向量中没有多余的。在二维情况中，如果在一组线性组合中有两个向量是共线的，那么这两个向量是线性相关的，只要不共线就是线性无关的。三维空间中，如果三个向量是共面的，即，其中一个向量可以通过其他两个向量组合得到，那么这三个向量就是线性相关的。总的来说，如果某个向量可以由其他向量的线性组合得到，那么这组向量就是线性相关的。线性相关是**描述一组向量之间的关系**。

## 1.8 张成的向量空间

- $\mathbb{R}^n$：$n$维的实坐标空间，$R$表示实数域。这个$n$维空间中包含了所有含有$n$个参数的实向量
- 例如：
  - $\mathbb{R}^1$：实数轴（一条直线）
  - $\mathbb{R}^2$：平面
  - $\mathbb{R}^3$：三维空间
- 特殊的向量空间：
  - $\mathbb{C}$：复数域上的空间
  - $\mathbb{M}$：包含所有$2*2$矩阵的空间
  - $\mathbb{F}$：包含所有实函数$f(x)$的
  - $\mathbb{Z}$：仅包含零向量的空间
  - $\mathbb{P}_n$：包含所有$n$阶多项式

一般的探讨只在二维或三维的实坐标空间中，即$\mathbb{R}^2,\mathbb{R}^3$。

- 定义： 给定一组向量 $v_1,v_2,...v_k$，它们的 张成空间（span） 定义为：$span\{v_1,v_2,...,v_k\}=\{c_1v_1+c_2v_2+...+c_kv_k \mid c_i \in R\}$
- 如果给定的向量组是线性无关的

也就是说：所有可能的线性组合构成的集合，就是这组向量的张成空间。张成空间 = 由这组向量能够“覆盖”的空间。

- 例：
  - 一维情况下，一个向量（不为零向量）张成的空间为一条直线，即实数轴
  - 二维情况下，给定两个不共线的向量$v_1,v_2$，它们的 span 是整个二维平面，即，两个不共线的非零向量通过线性组合可以张成一个二维平面。
  - 三维情况下：
    - 两个不平行的向量可以张成一个平面
    - 三个不共面的向量会张成整个三维空间
    - 如果三个向量是共面的，那么只会张成一个平面，如果这三个向量都共线，那么只会张成一根线

## 1.9 基与子空间

### 1.9.1 基

- 定义：在一个向量空间中，一组向量$\{v_1,v_2,...v_k\}$如果能满足：1. 线性无关 2. 张成整个空间，那么这组向量就是这个向量空间的一个基，这些向量叫做基向量。
- 维数：基向量的个数 = 空间的维数。例如，对于$\mathbb{R}^2$这个二维的向量空间，它的基必须有两个向量。
- 标准基：标准基是一组特定的基向量，对于$n$维实向量空间$R_n$，标准基是由 n 个向量组成的集合：$e_1,e_2,...,e_n$,每个向量$e_i$在第$i$个位置上为 1，其他位置为 0。标准基一般用字母$\hat{i},\hat{j},\hat{k}$表示，分别表示 x 轴上的基向量，y 轴上的，以及 z 轴上的。

### 1.9.2 子空间

- 设$V$是一个向量空间，$W$是$V$的一个子集。如果$W$本身在向量加法和数乘下也构成一个向量空间，那么$W$就叫做$V$的子空间。子空间必须是原空间的一部分，且必须对加法和数乘封闭。注：子空间**不一定是真子集**。子空间**必须包含原点**，故平移后的直线/平面不是子空间
- 一个非空子集$W \subseteq V$是子空间，当且仅当： - 零向量在其中：$\mathbf{0} \in W$ - 加法封闭：如果$u,v \in W$，那么$u+v \in W$ - 数乘封闭：如果$u \in W,c \in W$，那么$cu \in W$
  总的来说，封闭指的是子空间中的向量进行加法和乘法时，得到的结果仍然在子空间内。
- 子空间举例：
  - 在$\mathbb{R}^2$：
    - 原点{0}
    - 过原点的一条直线
    - 整个平面$\mathbb{R}^2$
  - 在$\mathbb{R}^3$：
    - 原点{0}
    - 过原点的一条直线
    - 过原点的一个平面
    - 整个平面$\mathbb{R}^3$

线性方程组的解集往往就是一个子空间

---

# 2 矩阵的基本运算

## 2.1 矩阵定义

- 定义：由$m*n$个数$a_{ij}$按照 m 行 n 列排列成的矩形数表：$$A=(a_{ij})_{m*n} = \begin{bmatrix}
  a_{11}&a_{12}&...&a_{1n}\\
  a_{21}&a_{22}&...&a_{2n} \\
  \vdots&\vdots&\ddots&\vdots\\
  a_{m1}&a_{m2}&...&a_{mn} 
\end{bmatrix}$$
- 元素：矩阵中第$i$行第$j$列的数$a_{ij}$
- 维度：矩阵的大小由行数$m$和列数$n$决定，称为$m*n$矩阵
- 对角线：
  - 主对角线：左上元素到右下元素的对角线
  - 副对角线：右上元素到左下元素的对角线
  - 迹：方阵主对角线上所有元素的和
  - 如果不是方阵，主对角线会在碰到最下面的那条边时结束，副对角线同理
- 特殊矩阵：
  - 行向量（$1*n$）和列向量（$m*1$）
  - 方阵：行数和列数相等的矩阵
  - 零矩阵：所有元素都是 0
  - 对角矩阵：主对角线以外都是 0 的矩阵，主对角线上可以是任何实数
  - 上三角矩阵：除了主对角线上方（包括主对角线上的元素）的元素以外都是 0 的方阵
  - 下三角矩阵：同上相反
  - 单位矩阵：对角线上的元素全是 1，其余元素全是 0，单位矩阵必须是方阵,通常记作$I$或者$E$：$$\begin{bmatrix}
		  1&0&0\\
		  0&1&0\\
		  0&0&1\\
		\end{bmatrix}$$

总结：

| 矩阵类型 | 定义               | 特殊性质                         |
| -------- | ------------------ | -------------------------------- |
| 零矩阵   | 所有元素为 0       | 加法/乘法的“零元”                |
| 单位矩阵 | 对角线全 1，其余 0 | 乘法的“单位元”                   |
| 对角矩阵 | 非对角线全 0       | 运算简便，零矩阵和单位矩阵是特例 |

注：一般大写字母表示矩阵

## 2.2 矩阵加减法及数乘

### 2.2.1 矩阵加减法

- 条件：只有两个矩阵的行数和列数完全相同才能相加或相减，运算规则为对应位置的元素相加/相减
- 性质：满足交换律，结合律，零矩阵$(A+0=A)$

### 2.2.2 矩阵的数乘

- 定义：一个数 k 与矩阵 A 相乘，就是把矩阵中的每个元素都乘以 k
- 性质：满足分配律$(k(A+B) = kA+kB)$，数乘结合律$((kl)A = k(lA))$，单位元：$1*A = A, 0*A=0$

## 2.3 矩阵乘法

### 2.3.1 线性变换

- “变换”可以类比成“函数”，即接受一个输入，产生一个输出。在线性代数中，通常意味着输入一个向量，输出一个新的向量。为什么要叫做变换而不是函数？这是因为要强调变换过程中的运动，即如何从原来的向量**运动**到新的向量的过程。
- 进一步，在二位情况下，如果我们推广这个运动到这个平面内的其他所有向量，会发现这个变换中所有的向量箭头都会运动到一个新的位置。如果换成网格视图，就会发现这个变换的效果就相当于对整个平面进行压缩，拉伸，和旋转。但是，要确保变换的“线性”，就必须要让变换前后的网格图中的每条直线依旧保持直线，平行且永不相交，以及，原点必须保持固定（原点移动的叫做仿射变换）。
- 矩阵可以看作是一种**变换**，或者**映射**，将原来的向量变换到新的向量上。而因为变换前后这个空间内的所有向量之间的相对位置都不变，我们就可以选择二维空间中的基向量，即$\hat{i}=(1，0),\hat{j}=(0，1)$作为参考，观察这两个向量在变换后的位置，这是因为这个空间中的任意向量都可以通过这两个基向量的线性组合得到。
- 下图展示了一个在$\mathbb{R}^2$中的一次线性变换（from:3b1b）：
- ![Pasted image 20250930105825.png](/src/assets/attachements/Pasted image 20250930105825.png)
- ![Pasted image 20250930105851.png](/src/assets/attachements/Pasted image 20250930105851.png)
- 由于我们是以基向量作为参照进行变换，并以此推测其他向量在变换前后的位置变化的，而且我们也知道基向量的初始位置，那么我们只需要得到基向量在变换后的位置，就能够完全描述这次变换了。例如，在上图的变换中，$\hat{i}$最终落在了$(1,-2)$，而$\hat{j}$最终落在了$(3,0)$。对于原空间中的任意向量$(x,y)$，将变换分别乘到$x$坐标和$y$坐标上，就得到了变换后的坐标：$$(x,y)\to x(1,-2) + y(3,0) = (1x+3y,-2x+0y)$$
- 如果我们把变换后的基向量的这两个坐标写在一起封装起来（习惯把它写成列向量的形式），就变成了一个矩阵：$$\begin{bmatrix}
		  1&3\\
		  -2&0\\
		\end{bmatrix}$$
  这个矩阵告诉我们，有这样一个变换，把$\hat{i}$移动到$(1,-2)$，把$\hat{j}$移动到$(3,0)$，并且所有的向量在变换后的位置都能通过原始位置乘上这个矩阵得出，这就是为什么说矩阵是一个**变换**，像一个函数一样，接受一个位置输入，输出一个新的位置。这样，我们就定义了矩阵和向量之间的乘法。二维矩阵的情况下，有如下一般定义：$$\begin{bmatrix}
		  a&b\\
		  c&d\\
		\end{bmatrix}\begin{bmatrix}
		  x\\
		  y\\
		\end{bmatrix} = x\begin{bmatrix}
		  a\\
		  c\\
		\end{bmatrix} + y\begin{bmatrix}
		  b\\
		  d\\
		\end{bmatrix} = \begin{bmatrix}
		  ax+by\\
		  cx+dy\\
		\end{bmatrix}$$
  在矩阵乘法中，一般习惯把矩阵写到左边，叫做**左乘**。这可以类比到函数的写法：f(x),也是把函数 f 写到自变量的左边，描述对自变量施加一种变化。也可以把对向量的这种变换写成如下的形式：$L(\overrightarrow{v})$，这样就更像一个函数了。
- 继续在二维情况下讨论，如果一个存在一个矩阵，使得变换后的$\hat{i}$和$\hat{j}$共线了会发生什么？在这种情况下，由于$\hat{i}$和$\hat{j}$是线性相关的，即其中一个向量可以由另一个向量线性组合得到，那么就意味着他们中间是有冗余的，即它们能张成的空间就不再是一个二维平面了，而是只能张成一根直线，并且原平面上的所有向量都会被压缩到这个平面上，相当于被“降维”了。在后续会详细解释这个过程，这个矩阵不是“满秩”的。**对于一个$m*n$的矩阵，其本质上是把$n$维空间中的所有基向量变换到$m$维空间中**，这样就能更好地理解非方阵的几何含义了。假设有矩阵$$A=\begin{bmatrix}
		  1&2&3\\
		  4&5&6\\
		\end{bmatrix}$$
  我们可以把它看成一个将三维空间压缩到一个二维的平面上。注意到原矩阵有三个列向量，分别对应$\mathbb{R}^3$中的三个基向量：$$A=\begin{bmatrix}
		  \hat{i}&\hat{j}&\hat{k}\\
		\end{bmatrix}$$
  但是这里面的列向量只有两个参数，意味着它所在的向量空间是二维的。以$\hat{i}$举例，它发生的变换是从$\{1,0,0\}$变换到$\{1,4\}$，其他的列向量也同理。

总的来说，一个$m \times n$矩阵$M$表示一个线性变换：
$$M:R^n→R^m$$
对于一个矩阵，其实际上是把“列数”大小的空间映射到“行数”大小的空间

### 2.3.2 矩阵乘法

- 接下来，我们需要将其推广到更一般的形式，即矩阵与矩阵的乘法。先给出矩阵乘法的数学定义：
  - $A$是一个$m×n$矩阵
  - $B$是一个$n×p$矩阵

则它们的乘积$C = AB$是一个$m×p$矩阵，元素计算规则为：$$c_{ij}=\sum_{k=1}^{n} a_{ik}b_{kj},(1\le i\le m,1\le j\le p)$$

- 条件：前矩阵的列数 = 后矩阵的行数，
- 结果：新矩阵的行数 = 前矩阵的行数，新矩阵的列数 = 后矩阵的列数
- 理解角度：
  - 行 × 列点积：每个元素是“行向量·列向量”的内积
  - 列组合视角：$AB$的每一列是$A$与$B$的对应列向量的线性组合
  - 线性变换视角：矩阵乘法对应于线性变换的复合（先做$B$，再做$A$
- 核心性质：
  - 结合律：$(AB)C = A(BC)$
  - 分配律：
    - 左分配：$A(B+C) = AB+AC$

对于矩阵乘法的条件，要保持输入输出的维度相匹配。本质上矩阵乘法是两个连续的变换的复合，先进行右边的变换，再进行左边的变换，因此，右侧变换后的维度必须匹配左侧输入的维度，即，右侧矩阵的行数必须等于左侧矩阵的列数。

例如，有矩阵$A$的大小是$m*n$，矩阵$B$的大小是$n*p$，那么$AB$是可以成立的。一般的，如果想要判断能否相乘，只需要左右看能不能消去：$m\times (n,n)\times p$。几何意义上来说，这代表着将空间由$\mathbb{R}^p$变换到$\mathbb{R}^n$，再从$\mathbb{R}^n$变换到$\mathbb{R}^m$。

### 2.3.3 左乘与右乘

在矩阵乘法中，由于矩阵表示一种变换，且这种变换涉及到维度的变化，一般的交换律不是普适的。一般左乘使用比较多，代表着对一个向量或另一个矩阵进行线性变换，如对$y=Ax$，这里称作$A$左乘$x$得到$y$。而右乘使用较少，但其与左乘实际上是等价的。如$v^TA$。通俗理解，左乘代表向量的变化，而右乘表示坐标系的变化。

## 2.4 矩阵的转置

给定一个$m*n$的矩阵：$$A=(a_{ij})_{m*n}$$
它的转置矩阵$A^T$是一个$n*m$的矩阵：$$(A^T)_{ij}=a_{ji}$$
即 **行列互换**：原矩阵的第$i$行变成转置矩阵的第$i$列。或者说，沿着主对角线，把上三角部分与下三角部分镜像。

- 例：$$A=\begin{bmatrix}
	a_{11}&a_{12}&a_{13}&a_{14}\\
	a_{21}&a_{22}&a_{23}&a_{24}\\
\end{bmatrix},A^T = \begin{bmatrix}
	a_{11}&a_{21}\\
	a_{12}&a_{22}\\
	a_{13}&a_{23}\\
	a_{14}&a_{24}\\
\end{bmatrix}$$

基本性质：

- 双重转置：$(A^T)^T = A$
- 加法与数乘：
  - $(A+B)^T = A^T+B^T$
  - $(kA)^T = kA^T$
- 乘法转置：$(AB)^T = B^TA^T$
- 逆矩阵关系（若$A$可逆）：$(A^{-1})^T = (A^T)^{-1}$
- 对称性：
  - 若$A^T = A$，则$A$是对称矩阵
  - 若$A^T = -A$，则$A$是反对称矩阵

## 2.5 矩阵的秩

- 定义：矩阵的秩是它的行向量（或列向量）中 **最大线性无关组的个数**，或最高阶非零子式的阶数。通过线性映射定义：若矩阵$A$表示一个线性变换 $T: \mathbb{R}^n \to \mathbb{R}^m$，则
  $$rank(A)=dim⁡(Im(T))$$即像空间的维数。
- 行秩 = 行向量张成空间的维数
- 列秩 = 列向量张成空间的维数
- 行秩 = 列秩，所以直接称为“矩阵的秩”
- 秩的取值可以是非零实数
- 满秩的矩阵的秩等于所在向量空间的维度

通俗解释，矩阵是一个变换，并且这个变换发生在一个向量空间中。而通过上文我们知道这个变换可能对所在的向量空间的维度产生变化，比如升维/降维/保持原有维度。而秩就是描述这种变换前后的维度变化的。如果说一个矩阵使得变换前后的维度保持不变，那么我们就说这个矩阵是“满秩”的。如果一个矩阵使得原来的三维空间压缩到了二维上，从$\mathbb{R}^3$变换到$\mathbb{R}^2$，那么这个矩阵的秩就是 2。**秩表示矩阵所能“覆盖”的空间维度，秩也等于线性变换的“有效维度”**

与线性方程组的关系：对于线性方程组 $Ax=b$：

- 若 $\text{rank}(A) = \text{rank}([A|b]) = n$ → 唯一解
- 若 $\text{rank}(A) = \text{rank}([A|b]) < n$ → 无穷多解
- 若 $\text{rank}(A) < \text{rank}([A|b])$ → 无解

## 2.6 可逆矩阵与逆矩阵运算

对于一个$n*n$大小的方阵$A$，如果能找到另一个$n*n$大小的方阵$B$，使得（$I_n$是单位矩阵，大小也为$n$）：$$AB=BA=I_n$$
那么$A$就是一个可逆矩阵，$B$称为$A$的逆矩阵，记作$A^{-1}$。如果不存在符合条件的$B$，那么$A$就不是一个可逆矩阵，又称为奇异矩阵。

判定：

- $\det(A) \neq 0$
- $\text{rank}(A) = n$（满秩）
- 线性方程组$Ax = b$对任意$b$都有唯一解
- $A$可以通过有限次初等行变换化为单位矩阵

性质：

- 逆矩阵的唯一性：如果矩阵的逆存在，那么其逆矩阵$A^{-1}$是唯一的
- $(A^{-1})^{-1} = A$
- $(AB)^{-1} = B^{-1} A^{-1}$
- $(A^T)^{-1} = (A^{-1})^T$
- $(kA)^{−1}=\frac{1}{k}A^{-1}(k≠0)$
- $\det(A^{-1})=\frac{1}{\det(A)}$

求逆的方法（后面会讨论）：

1. 伴随矩阵法：$$A^{-1} = \frac{1}{\det(A)}A^{*}$$
   其中$A^{*}$是$A$的伴随矩阵
2. 初等变换法（高斯–Jordan 消元）： 把 $[A \mid I]$通过初等行变换化为$[I \mid A^{-1}]$。
3. 分解法：如 LU 分解、QR 分解、SVD 等（更适合数值计算）。

意义：

- 解线性方程组：若$A$可逆，则$Ax=b \Rightarrow x = A^{-1} b$。
- 几何意义：矩阵是一个对向量空间的变换，那么逆矩阵就是找到一个矩阵，能够使得这两个矩阵的效果相抵消。如，一个矩阵的作用是把二维平面顺时针旋转 90 度，那么它的逆矩阵就是把这个平面逆时针旋转 90 度。这也就解释了为什么逆矩阵要求是方阵，因为非方阵意味着升维/降维，而这也意味着丢失信息，所以不能求逆矩阵。

## 2.7 矩阵的初等变换与初等矩阵

### 2.7.1 初等矩阵

- 定义：初等矩阵是指将单位矩阵$I$经过一次变换形成的矩阵
- 作用：
  - 左乘初等矩阵：对原矩阵做一次初等行变换
  - 又称初等矩阵：对原矩阵做一次初等列变换
- 三类初等矩阵：

1. 行交换矩阵，由单位矩阵交换两行得到：$$E=\begin{bmatrix}
	0&1\\
	1&0\\
\end{bmatrix}$$
   左乘某矩阵，相当于交换它的两行。
2. 行倍乘矩阵：由单位阵某一行乘以$k \neq 0$得到：$$E=\begin{bmatrix}
	k&0\\
	0&1\\
\end{bmatrix}$$
   左乘某矩阵，相当于把第一行乘以$k$。
3. 行倍加矩阵：由单位阵某一行加上另一行的$k$倍得到：$$E=\begin{bmatrix}
	1&0\\
	k&1\\
\end{bmatrix}$$
   左乘某矩阵，相当于“第 2 行 + k × 第 1 行”。

性质：

1. 初等矩阵都是可逆矩阵，其逆矩阵仍然是初等矩阵。
2. 任意可逆矩阵$A$，都可以表示为若干个初等矩阵的乘积：
   $A = E_1 E_2 \cdots E_k$
3. 用初等变换可以求逆矩阵：把$[A \mid I]$通过初等行变换化为$[I \mid A^{-1}]$。

应用场景：

- 解线性方程组：高斯消元法就是不断做初等行变换。
- 求逆矩阵：通过初等变换把$A$化为单位阵。
- 研究矩阵等价：矩阵$A$与$B$行等价 ⇔ 存在可逆矩阵$P$，使得$PA = B$。

### 2.7.2 矩阵的初等变换

初等变换分为**行变换**与**列变换**。以下是三种初等行变换：

1. 交换两行：把第$i$行与第$j$行对调。
2. 数乘某一行：用非零常数$k$乘以某一行。
3. 倍加某一行到另一行：把某一行的$k$倍加到另一行。
   列变换相似，不过是作用在列上

## 2.8 基础矩阵变换

### 2.8.1 剪切矩阵

在$\mathbb{R}^2$中，如果有一个矩阵，使得一个基向量不变的情况下，改变另外一个基向量的位置，造成整个空间产生“倾斜”的效果，称为一个剪切矩阵。直观表现为固定平行四边形的一边，改变另外一对边。剪切矩阵定义如下：$$shear_x(s)=\begin{bmatrix}
	1&s\\
	0&1\\\end{bmatrix},shear_y(s)=\begin{bmatrix}
	1&0\\
	s&1\\\end{bmatrix}$$

### 2.8.2 缩放矩阵

在$\mathbb{R}^2$中，如果有一个矩阵将空间沿着坐标轴缩放，那么称为一个缩放矩阵，具体形式为：$$scale(s_x,s_y) = \begin{bmatrix}
	s_x&0\\
	0&s_y\\\end{bmatrix}$$

### 2.8.3 旋转矩阵

首先，需要了解手性的概念：

> 手性：手性指的是自然界中的一种对称特点，简单来说，如果一个物体与其镜像是不同的，则其被称为“手性的”。如人的右手镜像后是左手，而左右手不是一样的，故人的手是“手性的”

本章从旋转矩阵的二维含义出发，拓展到三维，更高维度的旋转矩阵也可以被推导出来。首先，旋转矩阵有两种功能：**坐标系旋转和向量旋转**，这分别对应着**右乘**和**左乘**得到的结果。

#### 2.8.3.1 坐标系旋转（右乘）

设有向量空间$\mathbb{R}^2$中有一个平面直角坐标系$A$以及一个点$P=(p_{Ax},p_{Ay})$。现在将坐标系$A$旋转一定角度，但是保持点$P$的位置不变，如图所示：![IMG_5B5A364358C4-1.jpeg](/src/assets/attachements/IMG_5B5A364358C4-1.jpeg)
旋转后点$P$在坐标系$B$中的位置为$p_{Bx},p_{By}$，那么可以得到$p_{Ax},p_{Ay}$与$p_{Bx},p_{By}$的关系是：$$p_{Ax}=p_{Bx}\cos({\alpha})-p_{By}\sin({\alpha})$$
$$p_{Ay}=p_{Bx}\sin({\alpha})+p_{By}\cos({\alpha})$$
证明如下：![IMG_0A01195D23EC-1.jpeg](/src/assets/attachements/IMG_0A01195D23EC-1.jpeg)
也可以将以上公式写成矩阵形式：$$\begin{bmatrix}
	p_{Ax}\\
	p_{Ay}\\
\end{bmatrix}=\begin{bmatrix}
	\cos(\alpha)&-\sin(\alpha)\\
	\sin(\alpha)&\cos(\alpha)\\
\end{bmatrix}\begin{bmatrix}
	p_{Bx}\\
	p_{By}\\
\end{bmatrix}$$那么这样就可以得出旋转矩阵在二维中的形式，即将整个坐标系旋转一定角度的矩阵。这个矩阵把原空间中的所有向量都旋转了相同的角度，并且可以通过这个矩阵计算出旋转后的向量的位置。

- 直观理解：向量保持不懂，但是换了一个旋转后的坐标系去描述它

#### 2.8.3.2 旋转向量（左乘）

除了变换坐标，旋转矩阵还有一个功能是将向量旋转，如，将一个向量$p$旋转一定角度得到向量$p\prime$：![IMG_5B77175F0AE6-1.jpeg](/src/assets/attachements/IMG_5B77175F0AE6-1.jpeg)经过计算后，这部操作的旋转矩阵与上述的坐标变换得到的旋转矩阵是一样的：$$\begin{bmatrix}
	\cos(\alpha)&-\sin(\alpha)\\
	\sin(\alpha)&\cos(\alpha)\\
\end{bmatrix}$$

- 直观理解：坐标系不动，向量本身绕原点旋转

在三维中的情况更复杂一点，因为涉及三个轴的旋转。在后面的章节会详细讨论。

### 2.8.4 投影矩阵

考虑二维平面中的一个向量$\hat{v}$和一条线$L$，现在要计算$\hat{v}$在$L$上的投影的向量$\hat{v'}$，如图所示：![IMG_1CF97DC19E0E-1.jpeg](/src/assets/attachements/IMG_1CF97DC19E0E-1.jpeg)

这时，我们可以引入沿着直线$L$的一个向量$\hat{w}$，同时，由于$\hat{v'}$与$\hat{w}$共线，那么$\hat{v'}$与$\hat{w}$之间就是数乘的关系。首先计算$\hat{v}$与$\hat{w}$的点乘：$$\hat{v} \cdot \hat{w}$$并且，由于$\hat{v'}$与$\hat{w}$的缩放关系，我们就可以得出$\hat{v'}$的表达式：
$$\text{proj}_L(\hat{v})=\hat{v'}=\frac{\hat{v} \cdot \hat{w}}{\hat{w} \cdot \hat{w}}\hat{w}=\frac{\hat{v} \cdot \hat{w}}{{\left \|\hat{w}\right \|}^2}\hat{w}$$
如果我们进一步将$\hat{w}$设为单位向量，即长度为 1，记作$\hat{u}$，因为单位向量的模长为 1，故可以写成：$$\hat{v'}=(\hat{v} \cdot \hat{u})\hat{u}$$
于是就得到了投影矩阵$\text{proj}_L(\hat{v})=(\hat{v} \cdot \hat{u})\hat{u}$。其中$\hat{u}$是$L$上的单位向量。

### 2.8.5 对称矩阵

考虑相同的情况，不过这次，我们要计算向量$\hat{v}$关于线$L$对称的向量$\hat{v''}$，如图所示：![IMG_4A662B36C384-1.jpeg](/src/assets/attachements/IMG_4A662B36C384-1.jpeg)

容易得到：$$\text{ref}_L(\hat{v})=\hat{v''}=-(\hat{v}-\hat{v'})+\hat{v'}=2\hat{v'}-\hat{v}$$

## 2.9 伴随矩阵

### 2.9.1 余子式

在一个$n$阶方阵$A=(a_{ij})$中，去掉第$i$行和第$j$列后，剩下的$(n-1)$阶行列式，称为元素$a_{ij}$的余子式，记作$M_{ij}$。

例如：$$A=\begin{bmatrix}
	1&2&3\\
	4&5&6\\
	7&8&9\\
\end{bmatrix}$$
元素$a_{11}=1$的余子式是$$M_{11} = det\begin{pmatrix}
	5&6\\
	8&9\\
\end{pmatrix}=45-48=-3$$

### 2.9.2 代数余子式

元素$a_{ij}$的代数余子式$C_{ij}$定义为：
$$C_{ij}=(−1)^{i+j}M_{ij}$$
其中$(−1)^{i+j}$是一个符号因子：

- 当$i+j$为偶数时，符号为正
- 当$i+j$为奇数时，符号为负

### 2.9.3 伴随矩阵

伴随矩阵是方阵独有的概念。对于$n$阶方阵$A=(a_{ij})$，其伴随矩阵记作$\text{adj}(A)$或$A^*$。矩阵$A$的伴随矩阵就是由所有代数余子式组成的矩阵再转置：$$\text{adj}(A)=(C_{ij})^T$$
在求逆矩阵时，若$\det(A) \ne 0$，则：$$A^{-1} = \frac{1}{\det(A)}\text{adj}(A)$$

### 2.9.4 几何意义与总结

1. 余子式：去掉矩阵某一行和某一列后，剩下的子行列式。余子式就是在去掉某一维度后，剩下的向量所张成的“低一维体积”。例如：在三维矩阵中，某个元素的余子式对应于一个二维平行四边形的面积。可以理解为：余子式 = “去掉某个方向后，剩余向量的体积/面积”
2. 代数余子式：在几何上，它保证了行列式展开时，体积的“有向性”被正确记录。在三维中，代数余子式可以看作是“某个面作为底时，对应的有向面积”。可以理解为：代数余子式 = “带方向的低维体积”。
3. 伴随矩阵：伴随矩阵是由所有代数余子式组成的矩阵再转置。伴随矩阵与原矩阵满足关系：$$A \cdot \text{adj}(A) = |A|I$$
   这意味着：伴随矩阵在几何上是“原矩阵的逆变换 × 体积因子”。在三维中，伴随矩阵的列可以看作是 由原矩阵的两个列向量做叉积得到的法向量，代表了“正交方向上的面积向量”。更一般地，伴随矩阵把“低维体积（余子式）”拼接起来，形成一个能恢复整体体积（行列式）的工具。可以理解为：伴随矩阵 = “由低维体积（代数余子式）拼成的矩阵，它在几何上对应于原变换的‘体积加权逆’”

总结：

- 余子式：低一维的体积（面积、体积的截面）。
- 代数余子式：带方向的低一维体积。
- 伴随矩阵：把所有方向的“低维体积”组合起来，形成一个能恢复整体体积的矩阵。

## 2.10 分块矩阵

分块矩阵就是把一个大矩阵用横线和竖线划分成若干个“小矩阵”（子块），再把这些子块当作新的“元素”来研究。形式上：$$\begin{bmatrix}
	A_{11}&A_{12}&\cdots&A_{1n}\\
	A_{21}&A_{22}&\cdots&A_{2n}\\
	\vdots&\vdots&\ddots&\vdots\\
	A_{m1}&A_{m2}&\cdots&A_{mn}\\
\end{bmatrix}$$其中每个$A_{mn}$本身就是一个矩阵。分块矩阵的运算规则和普通矩阵是一致的，只是将其中的元素换成了子矩阵。
基本运算规则：

1. 加法：对应子块相加（前提是分块方式相同）。
2. 数乘：每个子块都乘以标量。
3. 乘法：与普通矩阵乘法类似，但“元素”换成了子块矩阵：$$C_{\alpha\beta} = \sum_\gamma A_{\alpha\gamma} B_{\gamma\beta}$$

4. 转置：子块转置并交换位置。

## 2.11 矩阵的子空间

### 2.11.1 列空间（像）

- 由矩阵$A$所有列向量张成的子空间，属于$\mathbb{R}^m$，写作$C(A)$。
- 几何意义：线性方程组$Ax=b$有解的情况是$b \in C(A)$
- 列空间的维数等于矩阵的秩
- 列空间 = 输出空间的张成空间（像空间）

### 2.11.2 行空间

- 由矩阵$A$所有行向量张成的子空间，属于$\mathbb{R}^n$，写作$R(A)$。
- 几何意义：与列空间相似，作用在转置矩阵上
- 行空间的维数也等于矩阵的秩
- 行空间 = 输入空间的张成空间（约束空间）

### 2.11.3 零空间（核）

- 所有满足$Ax=0$的解向量集合
- 所属空间：$\mathbb{R}^n$
- 几何意义：输入空间中被映射到零向量的方向（“消失的自由度”）。
- 维数：$n-r$，称为零度，或亏格。

### 2.11.4 左零空间

- 定义：所有满足$A^T y = 0$的向量集合
- 所属空间：$\mathbb{R}^m$
- 几何意义：与行空间正交的子空间；描述哪些输出方向无法由$A$的行组合得到。
- 维数：$m-r$。

直观理解，对于一个$m*n$的矩阵，在输入空间$\mathbb{R}^n$：

- 行空间 = 有效的输入方向
- 零空间 = 被压缩掉的方向
  在输出空间$\mathbb{R}^m$：
- 列空间 = 所有可能的输出
- 左零空间 = 永远无法到的的方向

## 2.12 核与像

设$T: V \to W$是一个线性变换，或者等价地，矩阵$A: \mathbb{R}^n \to \mathbb{R}^m$

- 核（Kernel / Null space）
  $\ker(T) = \{ v \in V \mid T(v) = 0 \}$
  即所有被映射到零向量的输入向量集合。核是$\mathbb{R}^n$的子集。
- 像（Image / Column space / Range）
  $\operatorname{Im}(T) = \{ T(v) \mid v \in V \} \subseteq W$
  即所有可能的输出向量的集合。像是$\mathbb{R}^m$的子集。

### 2.12.1 核与像的基

对于任意一个矩阵，要想计算其核的一组基，先将其化为 rref 的形式，即行最简形，直到完全不能再化简了，例如：

$$
A=\begin{bmatrix}
	1&2&0&0&2\\
	0&0&0&1&3\\
	0&0&0&0&0\\
\end{bmatrix}
$$

此时，$Ax=0$得出：
$$x_1+2x_2+2x_5=0, x_4+3x_5=0$$
其中第一列和第四列是主元，第二，三，五列是自由变量。令 $x_2=s,\ x_3=t,\ x_5=u$（自由变量），则
$x_1=-2s-2u,\quad x_4=-3u$

$$
\mathbf{x} = s\begin{bmatrix}-2\\1\\0\\0\\0\end{bmatrix} + t\begin{bmatrix}0\\0\\1\\0\\0\end{bmatrix} + u\begin{bmatrix}-2\\0\\0\\-3\\1\end{bmatrix}.
$$

得出核的基是：
$$\left\{ \begin{bmatrix}-2\\1\\0\\0\\0\end{bmatrix},\ \begin{bmatrix}0\\0\\1\\0\\0\end{bmatrix},\ \begin{bmatrix}-2\\0\\0\\-3\\1\end{bmatrix} \right\}$$

对于像空间的基，取原矩阵的第一和第四列即可组成像空间的基：
$$\left\{ \begin{bmatrix}1\\0\\0\end{bmatrix},\ \begin{bmatrix}0\\1\\0\end{bmatrix} \right\}$$

---

# 3 行列式

## 3.1 行列式的定义与性质

在矩阵对空间进行变换时，我们可以大致把这些变换分为两类：向内挤压原空间的和向外拉伸原空间的。如何去量化这种挤压和拉伸的程度呢？可以选取一块面积，并观察其在变换后的面积，以此得出挤压或拉伸的结论。以二维空间举例，对于$\mathbb{R}^2$中的基向量$\hat{i} = (1,0),\hat{j} = (0,1)$，他们围成的面积是$1*1 = 1$。假设有矩阵$$A=\begin{bmatrix}
	2&0\\
	0&1\\
\end{bmatrix}$$
这个矩阵把$\hat{i}$移动到了$(2,0)$的位置，直观上，把整个空间横向拉伸了两倍，再计算变换后基向量围成的面积就能得到$2*1 = 2$，大于原来的面积。（注意：有些矩阵在二维上是“剪切”的，如：![IMG_B1EE6DB5571D-1 1.jpeg](/src/assets/attachements/IMG_B1EE6DB5571D-1 1.jpeg)这时需要计算变换后的平行四边形的面积）对于这个变换，面积变化的倍数是 2，那么就定义这个矩阵的行列式$$det(A) = det(\begin{bmatrix}
	2&0\\
	0&1\\
\end{bmatrix}) = 2$$
如图所示：![IMG_8BAA8E5E2984-1 1.jpeg](/src/assets/attachements/IMG_8BAA8E5E2984-1 1.jpeg)
举例：

1. 对于一个矩阵：
   $$
   B=\begin{bmatrix}
   	4&2\\
   	2&1\\
   \end{bmatrix}
   $$
   由于它是列线性相关的，两个基向量在变换后共线，那么其张成的空间就变成了一根直线，自然，基向量围成的面积就变成了 0，行列式为 0
2. 对于一个旋转矩阵：
   $$
   C=\begin{bmatrix}
   	0&-1\\
   	1&0\\
   \end{bmatrix}
   $$
   这个矩阵将整个空间逆时针旋转了 90 度，但是旋转不改变面积，故行列式为 1

因此，只要检验一个矩阵的行列式是否为 0，就能判断其是否将空间压缩到了更低的维度上。

实际上，行列式也可以是负值。在二维空间中，这代表着将空间“翻转”了。对于之前的变换，不管是拉伸还是旋转，实际上$\hat{i}$一直在$\hat{j}$的右边。但是对于如下的矩阵：

$$
D=\begin{bmatrix}
	0&1\\
	1&0\\
\end{bmatrix}
$$

实际上将$\hat{i}$与$\hat{j}$交换了位置，相当于沿着$y=x$这条线镜像翻转了。严格来说，这样的变换称作“改变了空间取向”。如果用右手定理和叉乘验证，变换前$\hat{i} \times \hat{j}$的结果指向平面外，而变换后的结果指向平面里。当类似的空间取向改变时，行列式取负值，但是行列式的绝对值仍然能够体现变换前后面积的变化。对于更高维的情况也可以进行推广，如三维情况下行列式计算的就是体积。

性质：

- 转置不变：$\det(A^T) = \det(A)$
- 行列交换：交换两行（列），行列式变号
- 行列相同/成比例：若两行（列）相同或成比例，则行列式 = 0
- 数乘提取：某行（列）乘以$k$，行列式整体乘以$k$
- 行倍加不变：某行加上另一行的倍数，行列式不变
- 三角矩阵：上/下三角矩阵的行列式 = 主对角线元素的乘积
- 乘法性质：$\det(AB) = \det(A)\det(B)$
- 可逆性判定：$\det(A) \neq 0 \iff A$ 可逆

## 3.2 行列式的展开定理（Laplace 定理）

1. 按一行展开：对$n$阶行列式$D = \det(A)$，若按第$i$行展开：$$D=\sum_{j=1}^{n}a_{ij}C_{ij}$$其中$C_{ij}$是代数余子式
2. 按一列展开：按第$j$列展开：$$D=\sum_{i=1}^{n}a_{ij}C_{ij}$$表明：行列式可以按照任意一行/列展开，结果相同
3. 一般形式（按 4 行/列展开）：Laplace 定理的推广：行列式可以按任意 kk 行（或列）展开，展开式由对应的$k$阶子式与其代数余子式相乘组成。常用的是一行/一列展开，因为它能把$z$阶行列式降阶为$(n-1)$阶。

几何意义：展开定理体现了行列式的“递归结构”，高维体积可以分解为低维体积的组合。

## 3.3 行列式与矩阵可逆性的关系

- 基本结论：对于一个$n \times n$的方阵$A$：$$\det(A) \ne 0 \Leftrightarrow A可逆$$$$\det(A) = 0 \Leftrightarrow A不可逆（奇异矩阵）$$
  从代数角度来说，行列式非零说明矩阵的行或列线性无关，矩阵满秩，存在唯一的逆矩阵，而行列式为零说明矩阵的行或列线性相关，秩小于$n$，矩阵不可逆。

从几何角度理解，行列式的绝对值表示线性变换对体积的缩放因子：

- $\det(A) \neq 0$：变换保持了空间的“体积”，只是拉伸/压缩/翻转，仍然可以逆转。
- $\det(A) = 0$：变换把空间压缩到低维（如把三维压到平面），体积消失，信息丢失，无法逆转。

对于逆矩阵，其与行列式有如下的关系：若$A$可逆，则：$$A^{-1} = \frac{1}{\det(A)} \,\text{adj}(A)$$

其中 $\text{adj}(A)$是伴随矩阵，这说明行列式是逆矩阵存在的关键因子，
同时：$$det⁡(A^{−1})=\frac{1}{det⁡(A)}$$

## 3.4 秩-零度定理

对于一个线性变换
$$T: V \to W$$
其中$V$是有限维向量空间，成立关系：
$\dim(V) = \operatorname{rank}(T) + \operatorname{nullity}(T)$
等价地，对于一个$m \times n$矩阵$A$：
$$\operatorname{rank}(A) + \operatorname{nullity}(A) = n$$

- 秩（rank）：像空间（值域）的维数，表示线性变换“保留下来的独立方向数”。
- 零度（nullity）：核空间（零空间）的维数，表示被映射到零向量的独立方向数。
- 定义域维数：原空间的总维数。

直观理解，对于一个变换，秩描述的是变换过后的维度，零度描述的是过程中消失的维度，那么原空间的维度 = “保留的” + “丢失的”

---

# 4 线性方程组

## 4.1 齐次与非齐次线性方程组

### 2.5.1 齐次线性方程组

对于一个线性方程组（齐次）：$$\begin{cases}
 \\a_{11}x_1+a_{12}x_2+...a_{1n}x_n=b_1
 \\a_{21}x_1+a_{22}x_2+...a_{2n}x_n=b_2
 \\\vdots
 \\a_{m1}x_1+a_{m2}x_2+...a_{mn}x_n=b_m
\end{cases}$$
可以写成如下的矩阵形式：
$$Ax=b$$
其中：

- $A = (a_{ij})_{m*n}$是系数矩阵
- $x = (x_1,x_2,...x_n)^T$是未知量列向量
- $b = (b_1,b_2,...b_n)^T$是常数列向量

### 2.5.2 增广矩阵

在系数矩阵$A$的右边加上一列常数列向量$b$就得到了增广矩阵：

$$
[A|b] = \begin{bmatrix}
	a_{11}&a_{12}&\cdots&a_{1n}&b_1\\
	a_{21}&a_{22}&\cdots&a_{2n}&b_2\\
	\vdots&\vdots&\ddots&\vdots&\vdots\\
	a_{m1}&a_{m2}&\cdots&a_{mn}&b_n\\
\end{bmatrix}
$$

这就是增广矩阵，作用是把方程组的系数和常数项合并到一个矩阵中，方便用高斯消元法等方法统一处理。

## 4.2 高斯消元法与行最简形（RREF）

最早提出通过高斯消元法将线性方程组得到的增广矩阵化为行最简形，就能直接看出方程组的解。

高斯消元法的内容就是对得到的增广矩阵进行行变换，使得其变成一个除了主对角线和常数列上都是 0，主对角线上是 1 的一个矩阵，即行最简形。一般的做法是对矩阵的每一行标作$R_1,R_2,...,R_n$，并且对每一行可以替换为与其他行相加减或乘除的形式。

举例：对于线性方程组：$$\begin{cases}
 \\x+2y+z=2
 \\2x+y-z=1
 \\x-y+2z=3
\end{cases}$$
其增广矩阵$[A|b]$为：$$[A|b] = \begin{bmatrix}
	1&2&1&2\\
	2&1&-1&1\\
	1&-1&2&3\\
\end{bmatrix}$$对其进行高斯消元：

$$
\begin{bmatrix}
	1&2&1&2\\
	2&1&-1&1\\
	1&-1&2&3\\
\end{bmatrix} \underrightarrow{R_2 = 2R_1-R_2} \begin{bmatrix}
	1&2&1&2\\
	0&3&3&3\\
	1&-1&2&3\\
\end{bmatrix} \underrightarrow{R_3 = R_1-R_3} \begin{bmatrix}
	1&2&1&2\\
	0&3&3&3\\
	0&3&-1&-1\\
\end{bmatrix}
$$

$$
\begin{bmatrix}
	1&2&1&2\\
	0&3&3&3\\
	0&3&-1&-1\\
\end{bmatrix} \underrightarrow{R_3 = R_2-R_3} \begin{bmatrix}
	1&2&1&2\\
	0&3&3&3\\
	0&0&4&4\\
\end{bmatrix}
$$

这一步得到的是行阶梯形矩阵，即下三角矩阵都是 0 的矩阵

$$
\begin{bmatrix}
	1&2&1&2\\
	0&3&3&3\\
	0&0&4&4\\
\end{bmatrix} \underrightarrow{R_3 = \frac{R_3}{4}} \begin{bmatrix}
	1&2&1&2\\
	0&3&3&3\\
	0&0&1&1\\
\end{bmatrix}
$$

$$
\begin{bmatrix}
	1&2&1&2\\
	0&3&3&3\\
	0&0&1&1\\
\end{bmatrix} \underrightarrow{R_2 = \frac{R_2}{3}-R_3} \begin{bmatrix}
	1&2&1&2\\
	0&1&0&0\\
	0&0&1&1\\
\end{bmatrix} \underrightarrow{R_1 = R_1-R_3-2R_2} \begin{bmatrix}
	1&0&0&1\\
	0&1&0&0\\
	0&0&1&1\\
\end{bmatrix}
$$

最终得到行最简形，可以直接解出方程组的解。

行最简形相当于原矩阵的“精简版”，只保留了最核心的线性结构，包括张成的空间等。

在行最简形中，每一行的第一个非零元素（化为 1）称为该行的主元，其所在的列对应的变量称为主元变量。剩余的不被任何主元直接约束的变量称为自由变量。换句话说，主元变量就是像空间的维度，表示变换后张成的空间，自由变量就是核空间的维度，即在变换中被压缩到零向量的方向。

## 4.3 解的结构（唯一解，无解，无穷多解）

一般通过矩阵的秩去判断线性方程组的解的结构。对于线性方程组$Ax=b$，增广矩阵记为$[A|b]$，其中$A$是$m*n$大小的系数矩阵。

1. 唯一解：$\text{rank}(A) = \text{rank}([A|b]) = n$
   - 系数矩阵满秩
   - 每个未知数都能被唯一确定
2. 无解：$\text{rank}(A)<\text{rank}([A∣b])$
   - 增广矩阵比系数矩阵多出“矛盾约束”
   - 出现类似$0 = d(d \neq 0)$的矛盾方程
3. 无穷多解：$\text{rank}(A)=\text{rank}([A∣b])<n$
   - 方程组有解，但约束不足
   - 存在自由变量，可以取任意值 → 解空间是一个线性子空间（或其平移）
4. 特殊情况：$Ax=0$
   - 永远至少有零解。
   - 若$\text{rank}(A)=n$，则只有零解（唯一解）。
   - 若$\text{rank}(A)<n$，则有无穷多解（存在非零解）。

几何角度理解：

- 唯一解：几条直线/平面在一点交汇。
- 无解：几条直线/平面互相平行或矛盾，没有交点。
- 无穷多解：几条直线/平面重合或交于一条直线/一个平面。

## 4.4 克莱姆法则

克莱姆法则是一种利用行列式来解线性方程组的方法，适用于未知数个数与方程个数相同，且系数矩阵可逆（行列式不为零）的情况。

考虑一个$n$元线性方程组：$Ax=b$
其中：

- $A$是$n×n$的系数矩阵
- $\mathbf{x} = (x_1, x_2, \dots, x_n)^T$是未知向量
- $\mathbf{b} = (b_1, b_2, \dots, b_n)^T$是常数向量
  如果$\det(A) \neq 0$，则每个未知数$x_i$可以表示为：
  $$x_i = \frac{\det(A_i)}{\det(A)}, \quad i = 1,2,\dots,n$$

其中：

- $\det(A)$是系数矩阵的行列式
- $A_i$是将矩阵$A$的第$i$列替换为常数向量$\mathbf{b}$后得到的新矩阵

举例：
解方程组：
$\begin{cases}2x + y = 5 \\ x - y = 1 \end{cases}$

1. 系数矩阵：
   $A = \begin{bmatrix} 2 & 1 \\ 1 & -1 \end{bmatrix}, \quad \mathbf{b} = \begin{bmatrix} 5 \\ 1 \end{bmatrix}$
2. 行列式：
   $\det(A) = (2)(-1) - (1)(1) = -2 - 1 = -3$
3. 计算$x$： 替换第 1 列：
   $$A_1 = \begin{bmatrix} 5 & 1 \\ 1 & -1 \end{bmatrix}, \quad \det(A_1) = (5)(-1) - (1)(1) = -5 - 1 = -6$$
   $$x = \frac{\det(A_1)}{\det(A)} = \frac{-6}{-3} = 2$$

4. 计算$y$： 替换第 2 列：
   $$A_2 = \begin{bmatrix} 2 & 5 \\ 1 & 1 \end{bmatrix}, \quad \det(A_2) = (2)(1) - (5)(1) = 2 - 5 = -3$$
   $$y = \frac{\det(A_2)}{\det(A)} = \frac{-3}{-3} = 1$$

解为：$(x,y)=(2,1)$

## 4.5 非齐次线性方程组

和齐次方程组$Ax=0$相对应，它的形式是：
$$Ax=b,b≠0$$
定义：

- 齐次方程组：常数项全为 0，即$Ax=0$。
- 非齐次方程组：常数项不全为 0，即$Ax=b$，其中$b$是已知向量。

非齐次线性方程组的解通过秩来判断（秩-零度定理在这里发挥作用）：

- 设$A$为系数矩阵，$(A|b)$为增广矩阵。
  - 若 $\operatorname{rank}(A) \neq \operatorname{rank}(A|b)$，则无解。
  - 若 $\operatorname{rank}(A) = \operatorname{rank}(A|b) = n$（未知数个数），则唯一解。
  - 若 $\operatorname{rank}(A) = \operatorname{rank}(A|b) < n$，则无穷多解。

直观理解，在二维中，齐次方程组的解可能是一条过原点的直线，非齐次方程组的解就是这条直线平移后得到的平行直线， 在高维空间：齐次解是一个过原点的子空间，非齐次解是它的一个平移（仿射子空间）。

---

# 6 特征值与特征向量

## 6.1 特征值与特征向量的定义

对于一个$n \times n$方阵$A$，如果存在非零向量$x$和标量$\lambda$，使得：
$$A x = \lambda x$$
则称 $\lambda$是矩阵$A$的一个特征值， $x$是对应的特征向量。
直观理解，这个公式的意思是：矩阵$A$作用在向量$x$上，只是把它放大（或缩小、翻转）了 $\lambda$倍，而方向没变。对于一个线性变换，空间里大部分的向量都会改变方向和模长，但是如果能找到一个向量，使得它的方向在变换前后不改变，只是伸长或者缩短了，那么久称其为一个特征向量，伸长或缩短的倍数叫做特征值，如果特征值为负就代表方向被反转了，有点像行列式，而特征值为 0 说明被缩短成了一个点。

要求特征值，需要解特征方程：
$$\det(A - \lambda I) = 0$$
这是一个关于$\lambda$的$n$次多项式，称为特征多项式。解出的$\lambda$就是特征值，对应的特征向量是解方程$(A - \lambda I)x = 0$的非零解。

性质：

- 特征值之和 = 矩阵的迹（主对角线元素之和）。
- 特征值之积 = 矩阵的行列式。
- 不同特征值对应的特征向量线性无关。
- 若矩阵可对角化，则存在一组基底由特征向量组成。

## 6.2 特征多项式

对于一个$n \times n$方阵$A$，其 特征多项式定义为：
$$p_A(\lambda) = \det(\lambda I - A)$$
这是一个关于$\lambda$的$n$次多项式，其根就是矩阵$A$的特征值。对其进行展开可以得到：
$$p_A(\lambda) = \lambda^n + c_{n-1}\lambda^{n-1} + \cdots + c_1 \lambda + c_0$$
其中系数与矩阵的迹、行列式等有关。

直观理解，特征多项式是把“矩阵作用下哪些方向保持不变”转化为“求一个多项式的根”的工具，它把线性代数问题转化为代数方程问题。特征多项式的根就是矩阵的全部特征值，所以说它相当于是把矩阵的全部特征值打包起来放到一个多项式里。但它不直接包含特征向量的信息。特征向量需要在找到特征值之后，再去解对应的方程$(A-\lambda I)x=0$才能得到。

性质：

- 特征值之和 = 矩阵的迹（主对角线元素之和）。
- 特征值之积 = 矩阵的行列式。
- 相似不变性：若$B = P^{-1}AP$，则$p_B(\lambda) = p_A(\lambda)$。
- 根与代数重数：每个特征值的代数重数等于它在特征多项式中作为根出现的次数。
- 凯莱–哈密顿定理：矩阵$A$总满足自己的特征多项式，即$p_A(A)=0$

## 6.3 特征基

在求出特征向量后，如果该矩阵有足够多的线性无关的特征向量，可以组成一个基$\{v_1, v_2, \dots, v_n\}$，那么这组基就叫做特征基。在这个基下，矩阵 AA 的表示形式是一个对角矩阵：
$$P^{-1}AP = D$$
其中$P$的列就是特征向量，$D$的对角线是对应的特征值。如果特征向量不足，即几何重数小于代数重数，就不能形成完整的特征基。

## 6.4 特征子空间

对于一个矩阵$A$和它的某个特征值$\lambda$：
$$E_\lambda = \{\,x \in \mathbb{R}^n \mid (A - \lambda I)x = 0 \,\}$$
这个集合就是特征子空间，它包含了所有对应于特征值$\lambda$的特征向量以及零向量。它是一个向量子空间。直观理解，特征子空间就是把所有线性变换中“不变的方向”连同它们的线性组合放在一起，就形成了一个子空间。 换句话说，特征子空间 = 某个特征值对应的所有“不变方向”的集合。

例如，设
$$A = \begin{bmatrix}2 & 0 \\ 0 & 3\end{bmatrix}$$

- 特征值$\lambda_1=2$，对应方程$(A-2I)x=0$，解得特征向量方向$(1,0)$。 → 特征子空间$E_2 = \text{span}\{(1,0)\}$，就是 x 轴。
- 特征值$\lambda_2=3$，对应方程$(A-3I)x=0$，解得特征向量方向$(0,1)$。 → 特征子空间$E_3 = \text{span}\{(0,1)\}$，就是 y 轴。

所以这个矩阵的两个特征子空间就是 x 轴和 y 轴。

更一般的情况，如果一个特征值只有一个方向的特征向量，那么特征子空间是一条直线（一维）。如果一个特征值对应多个线性无关的特征向量 → 特征子空间可能是一个平面，甚至更高维的子空间。

## 6.5 代数重数与几何重数

- 代数重数（Algebraic Multiplicity） 某个特征值$\lambda$在特征多项式里作为根出现的次数。 它是“代数上出现了几次”。
- 几何重数（Geometric Multiplicity） 特征值$\lambda$对应的特征子空间的维数，也就是解方程 $(A-\lambda I)x=0$时能找到多少个线性无关的特征向量。 它是“几何上有多少个独立方向”。
  关系： 对任意矩阵，几何重数 ≤ 代数重数。 矩阵可对角化的充要条件是：**每个特征值的几何重数 = 代数重数**，并且所有特征向量加起来能凑满整个空间的基。

直观理解，一般矩阵作用在向量上会改变方向，但是如果能找到一组基底，使得矩阵在这些基底方向上只做“拉伸/压缩”，那在这组基下矩阵就变成了对角矩阵。而对角化就是“找到合适的坐标系，让矩阵变得最简单”。

## 6.6 相似矩阵与对角化

### 6.6.1 相似矩阵

定义：若存在可逆矩阵$P$，使得
$$B = P^{-1}AP$$
则称矩阵$A$与$B$相似，记作$A \sim B$。

相似矩阵与原矩阵共享的特征：

- 相同的特征多项式
- 相同的特征值（含代数重数）
- 相同的迹（对角线和）
- 相同的行列式
- 相同的秩
- 相似关系是等价关系（自反、对称、传递）

直观理解：相似矩阵表示**同一个线性变换在不同基下的表示形式**，或者，对于相同的一个线性变换，**用不同的坐标系去描述它**。

### 6.6.2 对角矩阵与对角化

对角矩阵（Diagonal Matrix）： 一个方阵，如果除了主对角线上的元素外，其余位置全是 0，那么它就是对角矩阵，主对角线上可以是任意元素。注意，零矩阵和单位矩阵也是对角矩阵。

对角化：如果矩阵$A$与某个对角矩阵$D$相似，即
$$A = P D P^{-1}$$
其中$D$是对角矩阵，$P$可逆，则称$A$可对角化。

它的几何意义是，找到一组合适的基，使得线性变换在这组基下的表示最简单（对角矩阵只在主对角线上有特征值）。对角化就是相似矩阵的一种特殊情况。在这种情况下，对于同一个线性变换的矩阵的表示是对角形式

对角化的条件

- 充要条件：矩阵$A$有$n$个线性无关的特征向量，等价于：每个特征值的**几何重数** = **代数重数**。
- 充分条件：若$A$有$n$个互不相同的特征值，则一定可对角化。
- 特殊情况：实对称矩阵一定可正交对角化（特征向量可取为正交基，$P$可取为正交矩阵）。

对角化的步骤

1. 求特征多项式$\det(A-\lambda I)=0$，解出特征值。
2. 对每个特征值，解$(A-\lambda I)x=0$，求特征向量。
3. 检查是否能得到$n$个线性无关的特征向量。
4. 若能，构造$P=[x_1,x_2,\dots,x_n]$，$D=\mathrm{diag}(\lambda_1,\dots,\lambda_n)$。 得到$A=PDP^{-1}$。

举例：考虑矩阵：
$$A = \begin{bmatrix}2 & 0 \\ 0 & 3\end{bmatrix}$$

- 特征多项式：$(\lambda-2)(\lambda-3)$，特征值是$\lambda_1=2, \lambda_2=3$。
- 每个特征值的代数重数 = 1。
- 解$(A-2I)x=0$，得到特征向量方向$(1,0)$。
- 解$(A-3I)x=0$，得到特征向量方向$(0,1)$。
- 每个特征值的几何重数也 = 1。
- 两个特征向量线性无关，正好构成二维空间的基。

于是$A$已经是对角矩阵，说明它在标准基下就表现为“x 方向拉伸 2 倍，y 方向拉伸 3 倍”。

另外一个例子：
$$B = \begin{bmatrix}1 & 1 \\ 0 & 1\end{bmatrix}$$

- 特征多项式：$(\lambda-1)^2$，特征值只有$\lambda=1$，代数重数 = 2。
- 解$(B-I)x=0$，得到特征向量方向只有$(1,0)$，几何重数 = 1。
- 因为几何重数 < 代数重数，矩阵$B$不可对角化。

几何直观：这个矩阵不是单纯的拉伸，而是“剪切”，它把$(0,1)$推向$(1,1)$，所以没有办法找到两条互相独立的“不变方向”。

---

# 7 内积与正交

## 7.1 定义与正交矩阵

### 7.1.1 内积与正交

- 内积定义：在$\mathbb{R}^n$中，两个向量
  $$a=(a_1,\dots,a_n), \quad b=(b_1,\dots,b_n)$$
  的内积（点积）为
  $$a \cdot b = \sum_{i=1}^n a_i b_i$$
  性质：
- 对称性：$a \cdot b = b \cdot a$
- 线性性：$(ka)\cdot b = k(a\cdot b)$
- 分配律：$(a+b)\cdot c = a\cdot c + b\cdot c$
- 正定性：$a\cdot a \geq 0$，且等号成立当且仅当$a=0$
- 几何意义：$a \cdot b = |a||b|\cos\theta$

内积刻画了两个向量的夹角关系。

- 正交定义：若$a \cdot b = 0$，则称向量$a$与$b$正交（垂直）。
- 正交向量组：每一组向量都两两正交。
- 标准正交向量组：在正交的基础上，每个向量长度为 1，即为单位向量。

对于两个向量，要计算它们的夹角的余弦值，可以从内积的公式中得出：$$cos(\theta)=\frac{\hat{v} \cdot \hat{w}}{\|\hat{v} \| \|\hat{w}\|}$$

### 7.1.2 正交矩阵

定义：矩阵$Q$满足
$$Q^T Q = I$$
则称$Q$为正交矩阵。（注意，正交矩阵形式上和逆矩阵容易搞混）

性质：

- 列向量（或行向量）组成一个标准正交向量组。
- 保长度、保角度（几何上是旋转/反射）。
- 逆矩阵就是转置：$Q^{-1}=Q^T$。

## 7.2 内积空间与范数

### 7.2.1 内积空间

在线性空间$V$上定义了一个“内积”运算
$$\langle u, v \rangle \in \mathbb{R} \ \text{或}\ \mathbb{C}$$
满足以下性质，则称$V$为内积空间：

1. 正定性：$\langle v,v \rangle \ge 0$，且等号成立当且仅当$v=0$。
2. 共轭对称性：$\langle u,v \rangle = \overline{\langle v,u \rangle}$。
3. 线性性：对第一个变量线性（实空间下对称双线性，复空间下半线性）。

几何意义：内积刻画了向量间的长度与夹角：
$$\langle u,v \rangle = \|u\|\|v\|\cos\theta$$
内积空间是欧几里得空间的推广。

### 7.2.2 范数

范数是向量“长度”的度量，记作$\|v\|$。

条件：

1. 非负性：$\|v\|\ge 0$，且$\|v\|=0 \iff v=0$。
2. 齐次性：$\|\alpha v\| = |\alpha|\|v\|$。
3. 三角不等式：$\|u+v\| \le \|u\| + \|v\|$。

内积空间和范数的关系是，在内积空间中，范数由内积诱导：
$$\|v\| = \sqrt{\langle v,v \rangle}$$
反过来，并不是所有范数都来自内积（例如$L^1$范数）。

## 7.3 正交向量与正交基

正交向量：在内积空间中，若两个向量$u, v$满足
$$\langle u, v \rangle = 0$$
则称$u$与$v$正交。

几何意义：在欧几里得空间中，正交即“垂直”。
性质：

- 非零正交向量必然线性无关。
- 正交性可以推广到多个向量：若集合中任意两向量都正交，则称为正交集。

正交基 (Orthogonal Basis)：若一个向量空间的基底由两两正交的向量组成，则称为正交基。 若这些基向量还都是单位向量，则称为标准正交基 (Orthonormal Basis)。

常见例子：$\mathbb{R}^n$中的标准基$\{e_1, e_2, \dots, e_n\}$。

## 7.4 正交投影

对于$\mathbb{R}^2$上的投影前面已经讨论过了，向量$\hat{v}$在线$l$上的投影是$\text{proj}_{\vec{u}}(\vec{v}) = (\vec{v} \cdot \vec{u}) \vec{u}$，其中$\vec{u}$是该方向上的单位向量。

更一般的形式是$\text{proj}_{\vec{a}}(\vec{v}) = \frac{\vec{v} \cdot \vec{a}}{\vec{a} \cdot \vec{a}} \vec{a}$
其中$\vec{a}$为该方向上的一个任意向量。

对其进行拓展，对于空间$S$中的向量$\vec{x}$和$S$中的子空间$V$，如果$\vec{u}_1,\vec{u}_2,...\vec{u}_m$是$V$的标准正交基，那么
$$\text{proj}_V{\vec{x}} = (\vec{x} \cdot \vec{u}_1) \vec{u}_1 + (\vec{x} \cdot \vec{u}_2) \vec{u}_2 + ... +(\vec{x} \cdot \vec{u}_m) \vec{u}_m$$

## 7.5 正交补空间

正交补空间，记作$V^\perp$，对于空间$V$，$V^\perp$是所有在$\mathbb{R}^n$中对$V$中所有向量都正交的向量的集合。
$$V^\perp=\{\vec{x} in \mathbb{R}^n : \vec{v} \cdot \vec{x} = 0 for\ all\ \vec{v} \in V\}$$
在二维空间中，假设有一个子空间是一条直线，那么其正交补空间就是所有垂直于这条直线的向量的集合。对于一个三维空间中的二维平面，它的正交补空间是这个平面的法线。

如果$V$是$\mathbb{R}^n$的子空间，那么存在$\text{dim}(V)+\text{dim}(V^\perp) = n$

## 7.6 Gram-Schmidt 正交化

Gram-Schmidt 正交化是在一个内积空间中，将一组线性无关的向量构造成一组正交基，或标准正交基。它的输入是一组线性无关向量 $\{v_1, v_2, \dots, v_n\}$（通常在 $\mathbb{R}^n$或$\mathbb{C}^n$）。

它的输出（正交）：向量$\{u_1, u_2, \dots, u_n\}$，满足两两正交，即$\langle u_i, u_j \rangle = 0(i≠ji \neq j)$。

如果要输出标准正交基，就再对每个$u_i$归一化得到$\{q_1, \dots, q_n\}$，使得 $\|q_i\| = 1$且$\langle q_i, q_j \rangle = 0$。

### 7.5.1 例 1

设有两个线性无关向量：
$$\quad v_1 = \begin{bmatrix}1 \\ 1\end{bmatrix}, \quad v_2 = \begin{bmatrix}1 \\ 0\end{bmatrix}$$
我们要用 Gram–Schmidt 把它们正交化。直接取：
$u_1 = v_1 = \begin{bmatrix}1 \\ 1\end{bmatrix}$
我们要把$v_2$中与$u_1$平行的部分去掉：
$$\text{proj}_{u_1}(v_2) = \frac{v_2 \cdot u_1}{u_1 \cdot u_1} u_1$$

$$v_2 \cdot u_1 = 1\cdot 1 + 0\cdot 1 = 1$$

$$u_1 \cdot u_1 = 1^2 + 1^2 = 2$$

$$\text{proj}_{u_1}(v_2) = \frac{1}{2}\begin{bmatrix}1 \\ 1\end{bmatrix} = \begin{bmatrix}0.5 \\ 0.5\end{bmatrix}$$

去掉投影：
$$u_2 = v_2 - \text{proj}_{u_1}(v_2) = \begin{bmatrix}1 \\ 0\end{bmatrix} - \begin{bmatrix}0.5 \\ 0.5\end{bmatrix} = \begin{bmatrix}0.5 \\ -0.5\end{bmatrix}$$
得到正交基
$$\{u_1, u_2\} = \left\{ \begin{bmatrix}1 \\ 1\end{bmatrix}, \begin{bmatrix}0.5 \\ -0.5\end{bmatrix} \right\}$$
接下来，为了得到标准正交基，我们可以把每个向量除以它的长度：
$$\|u_1\| = \sqrt{1^2+1^2} = \sqrt{2}$$
$$e_1 = \frac{1}{\sqrt{2}}\begin{bmatrix}1 \\ 1\end{bmatrix}$$
$$\|u_2\| = \sqrt{0.5^2+(-0.5)^2} = \sqrt{0.5} = \frac{1}{\sqrt{2}}$$
$$e_2 = \frac{1}{0.707}\begin{bmatrix}0.5 \\ -0.5\end{bmatrix} = \frac{1}{\sqrt{2}}\begin{bmatrix}1 \\ -1\end{bmatrix}$$
最终得到标准正交基：
$$\left\{ \frac{1}{\sqrt{2}}\begin{bmatrix}1 \\ 1\end{bmatrix}, \frac{1}{\sqrt{2}}\begin{bmatrix}1 \\ -1\end{bmatrix} \right\}$$

### 7.5.2 例 2

设$V$是$\mathbb{R}^n$的一个子空间，它的一组基是$\{\vec{v}_1,\vec{v}_2,\vec{v}_3\}$，现在需要使用 Gram–Schmidt 正交化把这组基转换成标准正交基$\{\vec{u}_1,\vec{u}_2,\vec{u}_3\}$。

1. 首先，求出$\vec{u}_1$：$\vec{u}_1=\frac{\vec{v}_1}{\|\vec{v}_1\|}$
2. 接下来，由$\vec{u}_1$得出$\vec{u}_2$，首先计算$\vec{v}_2^\perp$：$\vec{v}_2^\perp=\vec{v}_2-(\vec{v}_2 \cdot \vec{u}_1)\vec{u}_1$，$\vec{u}_2=\frac{\vec{v}_2^\perp}{\|\vec{v}_2^\perp\|}$
3. 最后，找到$\vec{u}_3$：$\vec{v}_3^\perp=\vec{v}_3-(\vec{v}_3 \cdot \vec{u}_1)\vec{u}_1-(\vec{v}_3 \cdot \vec{u}_2)\vec{u}_2$，$\vec{u}_3=\frac{\vec{v}_3^\perp}{\|\vec{v}_3^\perp\|}$
   这样就找到了一组标准正交基$\{\vec{u}_1,\vec{u}_2,\vec{u}_3\}$。

## 7.7 QR 分解

QR 分解是把一个矩阵分解为两个矩阵，本质上是用 Gram–Schmidt 正交化把矩阵的列向量转化为一组标准正交基（形成 $Q$），然后用上三角矩阵$R$来记录原始向量在这些正交基上的坐标。

## 7.8 正交对角化

如果一个矩阵$A$是一个对称矩阵，即$Q^TQ=I$，那么它就可以进行正交对角化。在确认满足对称矩阵的条件后，先找到该矩阵的所有特征值。对于每个特征值，找到$ker(M-\lambda I n)$的正交基（使用 Gram-Schmidt 方法），就可以得出正交对角化的一个矩阵。

例：考虑矩阵：

$$A = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}$$

这是一个实对称矩阵。对其求特征值：
解特征方程：

$$\det(A - \lambda I) = \begin{vmatrix} 2-\lambda & 1 \\ 1 & 2-\lambda \end{vmatrix} = (2-\lambda)^2 - 1=λ2−4λ+3=0= \lambda^2 - 4\lambda + 3 = 0$$

解得：

$$λ_1=3,λ_2=1$$

接下来求特征向量：

- 对$\lambda_1 = 3$： 解$(A - 3I)x = 0$：
  $$\begin{pmatrix} -1 & 1 \\ 1 & -1 \end{pmatrix} \begin{pmatrix}x \\ y\end{pmatrix} = 0$$

得到$x = y$，所以特征向量为：

$$v_1 = \begin{pmatrix}1 \\ 1\end{pmatrix}$$

- 对$\lambda_2 = 1$： 解$(A - I)x = 0$：

$$\begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix} \begin{pmatrix}x \\ y\end{pmatrix} = 0$$

得到$x = -y$，所以特征向量为：

$$v_2 = \begin{pmatrix}1 \\ -1\end{pmatrix}$$

注意$v_1$和$v_2$已经正交。我们将它们单位化：

$$u_1 = \frac{1}{\sqrt{2}}\begin{pmatrix}1 \\ 1\end{pmatrix}, \quad u_2 = \frac{1}{\sqrt{2}}\begin{pmatrix}1 \\ -1\end{pmatrix}$$

构造正交矩阵$Q$

$$Q = \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix}$$

得到对角矩阵$D$

$$D = \begin{pmatrix} 3 & 0 \\ 0 & 1 \end{pmatrix}$$

$$A = Q D Q^T$$

即：
$$\begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix} = \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix} \begin{pmatrix} 3 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix}^T$$

---

# 8 二次型

## 8.1 二次型的定义与矩阵表示

对于齐次线性的方程组，可以比较方便和直接地将其记录为矩阵形式，但是对于带有交叉项和二次项的多项式，就需要用到二次型将其表达为矩阵的形式，便于计算。
比如说，对于以下的多项式：$$ax^2+2bxy+cy^2$$可以将其写成矩阵的形式：$$\begin{bmatrix}
	x&y\\
\end{bmatrix}\begin{bmatrix}
	a&b\\
	b&c\\
\end{bmatrix}\begin{bmatrix}
	x\\
	y\\
\end{bmatrix}$$
计算这个式子后就能得到一开始的多项式：$ax^2+2bxy+cy^2$。进一步，可以将式子简化为$x^T A x$

那么我们就可以进一步得出二次型的定义：二次型就是一个关于多个变量的**二次齐次多项式**。 形式上：
$$Q(x) = x^T A x$$
其中：

- $x = (x_1, x_2, \dots, x_n)^T$是变量向量
- $A$是一个 **对称矩阵**（实二次型时）
- $Q(x)$就是二次型

展开形式：如果
$$A = \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{12} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{1n} & a_{2n} & \cdots & a_{nn} \end{bmatrix}$$
那么
$$Q(x) = a_{11}x_1^2 + a_{22}x_2^2 + \cdots + a_{nn}x_n^2 + 2a_{12}x_1x_2 + \cdots + 2a_{ij}x_ix_j$$
对角线元素对应平方项，非对角线元素对应交叉项。

在实际中，矩阵$A$是最重要的，甚至可以用来表示二次型。
在二维中，二次型就是形如
$$Q(x,y) = ax^2 + bxy + cy^2$$
的函数。其中，有$xy$的项叫做交叉项。

- 它的等值曲线往往是二次曲线（椭圆、双曲线、抛物线）。
- 在三维及更高维，它对应二次曲面（椭球面、双曲面等）。

## 8.2 配方法与标准型

### 8.2.1 配方法

配方法的目标是通过逐步“配方”，把二次型化为平方和的形式，消去交叉项。它的基本思路：

1. 选择一个含交叉项的变量对，例如$2a x_1 x_2$。
2. 将相关项组合成一个完全平方：
   $$ax_1^2 + 2b x_1 x_2 + c x_2^2 = a\left(x_1 + \frac{b}{a}x_2\right)^2 + \left(c - \frac{b^2}{a}\right)x_2^2$$
3. 依次对变量进行配方，直到所有交叉项消去，得到一个只含平方项的表达式，形式类似：
   $$f(x) = \lambda_1 y_1^2 + \lambda_2 y_2^2 + \dots + \lambda_n y_n^2$$
   其中$\{y_i\}$是经过线性变换后的新变量。

### 8.2.2 标准型

定义：通过配方法或合同变换，将二次型化为不含交叉项的对角形式：
$$f(x) \sim \lambda_1 y_1^2 + \lambda_2 y_2^2 + \dots + \lambda_n y_n^2$$
特点：

- 系数$\lambda_i$称为二次型的标准型系数。
- 这些系数的符号分布（正、负、零）决定了二次型的性质。

## 8.3 正定矩阵与判别准则（Sylvester 判别法）

### 8.3.1 正定矩阵

实对称矩阵$A$称为正定矩阵，若对任意非零向量$x \in \mathbb{R}^n$，都有：
$$x^\top A x > 0$$
类似地：

- 半正定矩阵：$x^\top A x \geq 0$
- 负定矩阵：$x^\top A x < 0$
- 半负定矩阵：$x^\top A x \leq 0$
- 不定矩阵：既可取正值也可取负值

### 8.3.2 特征值判断正定性

如果矩阵$A$的所有特征值都大于 0，则$A$正定。相似地，对于其他情况可以判断半正定，半负定，负定，不定。

### 8.3.3 Sylvester 判别法

核心结论： 实对称矩阵$A$$正定的充要条件是： 所有顺序主子式（leading principal minors）均为正数。

顺序主子式：
$$\Delta_k = \det(A_k), \quad k=1,2,\dots,n$$
其中$A_k$是矩阵$A$的前$k \times k$左上角子矩阵。

判别规则：

- 若$\Delta_1 > 0, \Delta_2 > 0, \dots, \Delta_n > 0$，则$A$正定。
- 若$\Delta_1 < 0, \Delta_2 > 0, \Delta_3 < 0, \dots$交替符号，则$A$负定。
- 若部分主子式为 0，则可能是半正定或半负定，需要进一步分析。
- 若符号不满足上述规律，则矩阵是不定的。

举例：设
$$A = \begin{bmatrix} 2 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 2 \end{bmatrix}$$
则有：

- $\Delta_1 = 2 > 0$
- $\Delta_2 = \det\begin{bmatrix}2 & -1 \\ -1 & 2\end{bmatrix} = 3 > 0$
- $\Delta_3 = \det(A) = 4 > 0$
  因此$A$是正定矩阵。

---

# 9 应用与扩展

## 9.1 最小二乘法

在很多情况下，我们需要解一个线性方程组：
$$Ax=b$$
其中：

- $A$是$m \times n$矩阵（通常$m > n$，方程数多于未知数）
- $x$是未知向量
- $b$是观测数据向量

如果方程组无解，我们希望找到一个近似解，使得误差最小。设有一个残差向量$r$使得向量

$$r = b - Ax$$
的长度最小，就可以找到这个近似解。这里的目标函数是：

$$f(x) = \|b - Ax\|^2 = (b - Ax)^T(b - Ax)$$

对 x 求导：

$$\nabla f(x) = -2A^T(b - Ax)$$

令其为零：

$$A^T(b - Ax) = 0$$

得到正规方程 (Normal Equation)：

$$A^T A x = A^T b$$

这就是最小二乘解的核心方程。几何意义上，最小二乘解就是把$b$正交投影到$A$的列空间上，得到最接近的点。

## 9.2 三维旋转矩阵

## 9.3 矩阵分解（LU，PLU，LDU 分解）

## 9.4 奇异值分解(SVD)

奇异值分解是一种将任意矩阵分解为三个矩阵乘积的方法，核心公式是：

$$A = U \Sigma V^T$$

其中$U$和$V$是正交矩阵，$\Sigma$是包含奇异值的对角矩阵。奇异值是矩阵$A^TA$或$AA^T$的特征值的平方根，通常按降序排列。

## 9.5 PCA(主成分分析)与数据降维
